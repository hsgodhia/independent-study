{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, pdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from itertools import ifilter\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 130000\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, pretrained=None):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        #these are by intent to learn separate embedding matrices, we return word_emb\n",
    "        self.word_emb = nn.Embedding(vocab_size, hid_dim)\n",
    "        if pretrained is not None:\n",
    "            self.word_emb.weight.data.copy_(pretrained)\n",
    "        \n",
    "        self.context_emb = nn.Embedding(vocab_size, hid_dim)\n",
    "        if pretrained is not None:\n",
    "            self.context_emb.weight.data.copy_(pretrained)        \n",
    "        \n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, wrd, cntxt, labels):\n",
    "        wrd_vec = self.word_emb(wrd) # N * 1 * D\n",
    "        cntxt_vec = self.context_emb(cntxt) # N * 5 * D\n",
    "        res = torch.bmm(wrd_vec, cntxt_vec.view(BATCH_SIZE, self.hid_dim, -1))\n",
    "        res = res.squeeze(1)\n",
    "        res = res * labels\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        # these are N * (1 + neg_exmpl) logsigmoid values\n",
    "        # for each mini-batch we have a probability score for the 5 contexts\n",
    "        # return res\n",
    "        \n",
    "        return (torch.sum(res)*-1.0)/res.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(wrd, k, mat, word2index):\n",
    "    if wrd not in word2index:\n",
    "        return None\n",
    "    vec = mat[word2index[wrd], :].unsqueeze(1)\n",
    "    othrs = torch.mm(mat, vec)\n",
    "    othrs, ind = torch.sort(othrs, 0, descending=True)\n",
    "    topk = ind[:k]\n",
    "    for i in range(topk.size()[0]):\n",
    "        print(index2word[topk[i][0]])\n",
    "\n",
    "def get_glovedict(glove_path):\n",
    "    vocab_d = set()\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            vocab_d.add(word)\n",
    "            \n",
    "    return vocab_d\n",
    "    \n",
    "def get_gloveready(glove_path, vocab_size, dim, word2index):\n",
    "    pretrained_weight = torch.FloatTensor(vocab_size, dim)\n",
    "    fnd = 0\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            if word in word2index:\n",
    "                ind = word2index[word]\n",
    "                pretrained_weight[ind, :] = torch.from_numpy(np.array(list(map(float, vec.split()))))\n",
    "                fnd += 1\n",
    "\n",
    "    print('Found {0} words with glove vectors, total was {1}'.format(fnd, vocab_size))\n",
    "    return pretrained_weight\n",
    "\n",
    "def process_lines(data):\n",
    "    pairs, vocab = set(), {}\n",
    "    for cn, l in enumerate(data):\n",
    "        dt = l.split(\"|||\")\n",
    "        score = float(dt[3].split(\" \")[1].split(\"=\")[1])\n",
    "        if score < 3.3:\n",
    "            continue\n",
    "        wrd1, wrd2 = dt[1], dt[2]\n",
    "        wrd1, wrd2 = wrd1.strip(), wrd2.strip()\n",
    "\n",
    "        if \".pdf\" not in wrd1 and \".pdf\" not in wrd2 and wrd1.isalpha() and wrd2.isalpha():\n",
    "            sc = editdist_score(wrd1, wrd2)\n",
    "            if sc > min(len(wrd1), len(wrd2))/2 + 2:\n",
    "                if wrd1 + \" \" + wrd2 not in pairs and wrd2 + \" \" + wrd1 not in pairs:\n",
    "                    pairs.add(wrd1 + \" \" + wrd2)\n",
    "                    if wrd1 not in vocab:\n",
    "                        vocab[wrd1] = 1\n",
    "                    else:\n",
    "                        vocab[wrd1] += 1\n",
    "\n",
    "                    if wrd2 not in vocab:\n",
    "                        vocab[wrd2] = 1\n",
    "                    else:\n",
    "                        vocab[wrd2] += 1\n",
    "\n",
    "    return pairs, vocab\n",
    "\n",
    "def get_vocab(min_freq, flName=None, lines=None):\n",
    "    if flName is not None:\n",
    "        with open(flName) as fp:\n",
    "            lines = fp.readlines()\n",
    "    \n",
    "    return process_lines(lines)\n",
    "\n",
    "def get_chunks(lines, cn):\n",
    "    chunks = []\n",
    "    chunk_size = len(lines)//cn\n",
    "    for i in range(0, chunk_size*cn + 1):\n",
    "        chunk = lines[i*chunk_size:i*chunk_size + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def editdist_score(p1, p2):\n",
    "    n, m = len(p1), len(p2)\n",
    "    dp = [[0 for x in range(m+1)] for x in range(n+1)]\n",
    "\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            if i == 0:\n",
    "                dp[0][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][0] = i            \n",
    "            elif p1[i-1] == p2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n",
    "    return dp[n][m]\n",
    "\n",
    "def filter_data(pairs, word2index):\n",
    "    new_pairs = set()\n",
    "    fp = open(\"ppdb-processed.txt\", \"w\")\n",
    "    for line in pairs:        \n",
    "        p1, p2 = line.split(\" \")\n",
    "        if p1 in word2index and p2 in word2index:\n",
    "            new_pairs.add(p1 + \" \" + p2)\n",
    "            fp.write(line)\n",
    "            fp.write(\"\\n\")\n",
    "            \n",
    "    fp.close()\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 21232 193269 21232\n"
     ]
    }
   ],
   "source": [
    "glove_path, dim, min_count, neg_exmpl = \"glove.6B.50d.txt\", 50, 1, 10\n",
    "g_vocab = get_glovedict(glove_path)\n",
    "pairs, tok_freq = get_vocab(min_count, flName=\"ppdb-2.0-l-lexical\")\n",
    "\n",
    "vocab = set(tok_freq.keys())\n",
    "vocab = vocab.intersection(g_vocab)\n",
    "word2index, index2word = {}, {}\n",
    "\n",
    "for wrd in vocab:\n",
    "    if tok_freq[wrd] >= min_count:\n",
    "        index2word[len(index2word)] = wrd\n",
    "        word2index[wrd] = len(index2word) - 1\n",
    "    else:\n",
    "        tok_freq[wrd] = 0\n",
    "\n",
    "pairs = filter_data(pairs, word2index)\n",
    "vocab_size = len(index2word)\n",
    "print(\"Data ready: {} {} {}\".format(vocab_size, len(pairs), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl = Word2Vec(vocab_size, dim)\n",
    "mdl.load_state_dict(torch.load('./mdl_skipgm.pth', map_location=lambda storage, loc: storage))\n",
    "w2vmat = torch.nn.functional.normalize(mdl.word_emb.weight.data.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young\n",
      "who\n",
      "friends\n",
      "fellow\n",
      "younger\n",
      "man\n",
      "parents\n",
      "friend\n",
      "couple\n",
      "boys\n"
     ]
    }
   ],
   "source": [
    "get_sim('young', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occur\n",
      "occurring\n",
      "occurs\n",
      "arise\n",
      "affect\n",
      "affected\n",
      "occurrence\n",
      "possibly\n",
      "cause\n",
      "due\n"
     ]
    }
   ],
   "source": [
    "get_sim('occur', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer\n",
      "winter\n",
      "beginning\n",
      "starting\n",
      "day\n",
      "during\n",
      "days\n",
      "year\n",
      "next\n",
      "years\n"
     ]
    }
   ],
   "source": [
    "get_sim('summer', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eating\n",
      "ate\n",
      "eaten\n",
      "eats\n",
      "cooked\n",
      "fish\n",
      "vegetables\n",
      "feed\n",
      "eggs\n"
     ]
    }
   ],
   "source": [
    "get_sim('eat', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "worry\n",
      "danger\n",
      "fears\n",
      "anger\n",
      "blame\n",
      "fearing\n",
      "worried\n",
      "threatening\n",
      "cause\n"
     ]
    }
   ],
   "source": [
    "get_sim('fear', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ho\n",
      "ai\n",
      "tu\n",
      "yo\n",
      "ya\n",
      "techs\n",
      "wow\n",
      "ok\n",
      "hurts\n"
     ]
    }
   ],
   "source": [
    "get_sim('hi', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n",
      "furious\n",
      "outraged\n",
      "enraged\n",
      "anger\n",
      "angered\n",
      "frustrated\n",
      "shouted\n",
      "loud\n",
      "shouting\n"
     ]
    }
   ],
   "source": [
    "get_sim('angry', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "worry\n",
      "danger\n",
      "fears\n",
      "anger\n",
      "blame\n",
      "fearing\n",
      "worried\n",
      "threatening\n",
      "cause\n"
     ]
    }
   ],
   "source": [
    "get_sim('fear', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary\n",
      "salaries\n",
      "payroll\n",
      "bonuses\n",
      "minimum\n",
      "pay\n",
      "payments\n",
      "guaranteed\n",
      "paid\n",
      "paying\n"
     ]
    }
   ],
   "source": [
    "get_sim('salary', 10, w2vmat, word2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
