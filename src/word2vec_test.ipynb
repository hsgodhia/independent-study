{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, pdb, csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from itertools import ifilter\n",
    "from random import randint\n",
    "from joblib import Parallel, delayed\n",
    "from torch.optim import lr_scheduler\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 130000\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, pretrained=None):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        #these are by intent to learn separate embedding matrices, we return word_emb\n",
    "        self.word_emb = nn.Embedding(vocab_size, hid_dim)\n",
    "        if pretrained is not None:\n",
    "            self.word_emb.weight.data.copy_(pretrained)\n",
    "        \n",
    "        self.context_emb = nn.Embedding(vocab_size, hid_dim)\n",
    "        if pretrained is not None:\n",
    "            self.context_emb.weight.data.copy_(pretrained)        \n",
    "        \n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, wrd, cntxt, labels):\n",
    "        wrd_vec = self.word_emb(wrd) # N * 1 * D\n",
    "        cntxt_vec = self.context_emb(cntxt) # N * 5 * D\n",
    "        res = torch.bmm(wrd_vec, cntxt_vec.view(BATCH_SIZE, self.hid_dim, -1))\n",
    "        res = res.squeeze(1)\n",
    "        res = res * labels\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        # these are N * (1 + neg_exmpl) logsigmoid values\n",
    "        # for each mini-batch we have a probability score for the 5 contexts\n",
    "        # return res\n",
    "        \n",
    "        return (torch.sum(res)*-1.0)/res.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(wrd, k, mat, word2index):\n",
    "    if wrd not in word2index:\n",
    "        return None\n",
    "    vec = mat[word2index[wrd], :].unsqueeze(1)\n",
    "    othrs = torch.mm(mat, vec)\n",
    "    othrs, ind = torch.sort(othrs, 0, descending=True)\n",
    "    topk = ind[:k]\n",
    "    for i in range(topk.size()[0]):\n",
    "        print(index2word[topk[i][0]])\n",
    "\n",
    "def get_score(wrd1, wrd2, mat, word2index):\n",
    "    return torch.dot(mat[word2index[wrd1],:], mat[word2index[wrd2], :])\n",
    "\n",
    "def get_glovedict(glove_path):\n",
    "    vocab_d = set()\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            vocab_d.add(word)\n",
    "            \n",
    "    return vocab_d\n",
    "    \n",
    "def get_gloveready(glove_path, vocab_size, dim, word2index):\n",
    "    pretrained_weight = torch.FloatTensor(vocab_size, dim)\n",
    "    fnd = 0\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            if word in word2index:\n",
    "                ind = word2index[word]\n",
    "                pretrained_weight[ind, :] = torch.from_numpy(np.array(list(map(float, vec.split()))))\n",
    "                fnd += 1\n",
    "\n",
    "    print('Found {0} words with glove vectors, total was {1}'.format(fnd, vocab_size))\n",
    "    return pretrained_weight\n",
    "\n",
    "def process_lines(data):\n",
    "    pairs, vocab = set(), {}\n",
    "    for cn, l in enumerate(data):\n",
    "        dt = l.split(\"|||\")\n",
    "        score = float(dt[3].split(\" \")[1].split(\"=\")[1])\n",
    "        if score < 3.3:\n",
    "            continue\n",
    "        wrd1, wrd2 = dt[1], dt[2]\n",
    "        wrd1, wrd2 = wrd1.strip(), wrd2.strip()\n",
    "\n",
    "        if \".pdf\" not in wrd1 and \".pdf\" not in wrd2 and wrd1.isalpha() and wrd2.isalpha():\n",
    "            sc = editdist_score(wrd1, wrd2)\n",
    "            if sc > min(len(wrd1), len(wrd2))/2 + 2:\n",
    "                if wrd1 + \" \" + wrd2 not in pairs and wrd2 + \" \" + wrd1 not in pairs:\n",
    "                    pairs.add(wrd1 + \" \" + wrd2)\n",
    "                    if wrd1 not in vocab:\n",
    "                        vocab[wrd1] = 1\n",
    "                    else:\n",
    "                        vocab[wrd1] += 1\n",
    "\n",
    "                    if wrd2 not in vocab:\n",
    "                        vocab[wrd2] = 1\n",
    "                    else:\n",
    "                        vocab[wrd2] += 1\n",
    "\n",
    "    return pairs, vocab\n",
    "\n",
    "def get_vocab(min_freq, flName=None, lines=None):\n",
    "    if flName is not None:\n",
    "        with open(flName) as fp:\n",
    "            lines = fp.readlines()\n",
    "    \n",
    "    return process_lines(lines)\n",
    "\n",
    "def get_chunks(lines, cn):\n",
    "    chunks = []\n",
    "    chunk_size = len(lines)//cn\n",
    "    for i in range(0, chunk_size*cn + 1):\n",
    "        chunk = lines[i*chunk_size:i*chunk_size + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def editdist_score(p1, p2):\n",
    "    n, m = len(p1), len(p2)\n",
    "    dp = [[0 for x in range(m+1)] for x in range(n+1)]\n",
    "\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            if i == 0:\n",
    "                dp[0][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][0] = i            \n",
    "            elif p1[i-1] == p2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n",
    "    return dp[n][m]\n",
    "\n",
    "def filter_data(pairs, word2index):\n",
    "    new_pairs = set()\n",
    "    fp = open(\"ppdb-processed.txt\", \"w\")\n",
    "    for line in pairs:        \n",
    "        p1, p2 = line.split(\" \")\n",
    "        if p1 in word2index and p2 in word2index:\n",
    "            new_pairs.add(p1 + \" \" + p2)\n",
    "            fp.write(line)\n",
    "            fp.write(\"\\n\")\n",
    "            \n",
    "    fp.close()\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_opposites():\n",
    "    oppos = {}\n",
    "    with open('sneha_antonyms.csv', 'r') as csvfile:\n",
    "        next(csvfile)\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        for row in data:\n",
    "            if row[1] == 'antonym' and float(row[2]) > 0.8:\n",
    "                wrd1, wrd2 = row[3], row[4]\n",
    "                wrd1 = wrd1.strip().lower()\n",
    "                wrd2 = wrd2.strip().lower()\n",
    "                \n",
    "                if wrd1 not in oppos:\n",
    "                    oppos[wrd1] = []\n",
    "                oppos[wrd1].append(wrd2)\n",
    "                if wrd2 not in oppos:\n",
    "                    oppos[wrd2] = []\n",
    "                oppos[wrd2].append(wrd1)\n",
    "    return oppos\n",
    "\n",
    "def init_train(glove_path, dim, min_count, neg_exmpl):\n",
    "    g_vocab = get_glovedict(glove_path)\n",
    "    pairs, tok_freq = get_vocab(min_count, flName=\"ppdb-2.0-l-lexical\")\n",
    "    opposites = load_opposites()\n",
    "    for wrd in opposites:\n",
    "        if wrd not in tok_freq:\n",
    "            tok_freq[wrd] = 0\n",
    "        tok_freq[wrd] += 1\n",
    "        \n",
    "        antos = opposites[wrd]\n",
    "        for awrd in antos:\n",
    "            if awrd not in tok_freq:\n",
    "                tok_freq[awrd] = 0\n",
    "            tok_freq[awrd] += 1\n",
    "            \n",
    "    vocab = set(tok_freq.keys())\n",
    "    vocab = vocab.intersection(g_vocab)\n",
    "\n",
    "    word2index, index2word = {}, {}\n",
    "    \n",
    "    for wrd in vocab:\n",
    "        if tok_freq[wrd] >= min_count:\n",
    "            index2word[len(index2word)] = wrd\n",
    "            word2index[wrd] = len(index2word) - 1\n",
    "        else:\n",
    "            tok_freq[wrd] = 0\n",
    "            \n",
    "    pairs = filter_data(pairs, word2index)\n",
    "    print(\"Data ready: {} {} {}\".format(len(index2word), len(pairs), len(vocab)))\n",
    "    return pairs, word2index, index2word, vocab, tok_freq, opposites\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 17585 103416 17585\n",
      "Data ready: 17585 103416 17585\n"
     ]
    }
   ],
   "source": [
    "glove_path, dim, min_count, neg_exmpl = \"glove.6B.50d.txt\", 50, 1, 10\n",
    "pairs, word2index, index2word, vocab, tok_freq, opposites = init_train(glove_path, dim, min_count, neg_exmpl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While copying the parameter named word_emb.weight, whose dimensions in the model are torch.Size([17585, 50]) and whose dimensions in the checkpoint are torch.Size([21530, 50]), ...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected tensor [17585 x 50] and src [21530 x 50] to have the same number of elements, but got 879250 and 1076500 elements respectively at /opt/conda/conda-bld/pytorch_1502006348621/work/torch/lib/TH/generic/THTensorCopy.c:86",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-9d518c045061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./mdl_skipgme6.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mw2vmat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_emb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2vmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0mown_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 print('While copying the parameter named {}, whose dimensions in the model are'\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected tensor [17585 x 50] and src [21530 x 50] to have the same number of elements, but got 879250 and 1076500 elements respectively at /opt/conda/conda-bld/pytorch_1502006348621/work/torch/lib/TH/generic/THTensorCopy.c:86"
     ]
    }
   ],
   "source": [
    "mdl = Word2Vec(len(index2word), dim)    \n",
    "mdl.load_state_dict(torch.load('./mdl_skipgme6.pth', map_location=lambda storage, loc: storage))\n",
    "w2vmat = mdl.word_emb.weight.data.cpu()\n",
    "wnorm = torch.norm(w2vmat, 2, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occur\n",
      "occurring\n",
      "affect\n",
      "arise\n",
      "localized\n",
      "due\n",
      "likely\n",
      "result\n",
      "possibly\n",
      "occurrences\n"
     ]
    }
   ],
   "source": [
    "get_sim('occur', 10, w2vmat/wnorm, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occur\n",
      "occurring\n",
      "arise\n",
      "affect\n",
      "possibly\n",
      "due\n",
      "normal\n",
      "localized\n",
      "outbreaks\n",
      "possible\n"
     ]
    }
   ],
   "source": [
    "get_sim('occur', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "shepherd\n",
      "slumped\n",
      "unshackled\n",
      "craftspeople\n",
      "detonation\n",
      "weighed\n",
      "presentations\n",
      "deception\n",
      "electrical\n"
     ]
    }
   ],
   "source": [
    "get_sim('eat', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "guidance\n",
      "propuestas\n",
      "fora\n",
      "centralized\n",
      "narrates\n",
      "pressured\n",
      "replace\n",
      "attracting\n",
      "disassociated\n"
     ]
    }
   ],
   "source": [
    "get_sim('fear', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ho\n",
      "ai\n",
      "ok\n",
      "wow\n",
      "techs\n",
      "oh\n",
      "wanna\n",
      "yo\n",
      "tu\n"
     ]
    }
   ],
   "source": [
    "get_sim('hi', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_sim('salary', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coast\n",
      "tetroxide\n",
      "fate\n",
      "bettering\n",
      "collecting\n",
      "indisputable\n",
      "reside\n",
      "grenades\n",
      "happy\n",
      "recorders\n"
     ]
    }
   ],
   "source": [
    "get_sim('coast', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8319517970085144"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('coast', 'shore', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7724562287330627"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('young', 'new', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05693214759230614"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('east', 'west', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8381562829017639"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('take', 'obtain', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.886364758014679"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('receive', 'give', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7134382724761963"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('shrinks', 'small', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shrink\n",
      "shrinking\n",
      "shrinks\n",
      "shrank\n",
      "shrunk\n",
      "increase\n",
      "slowing\n",
      "reduction\n",
      "reduce\n",
      "dramatically\n"
     ]
    }
   ],
   "source": [
    "get_sim('shrink', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monster\n",
      "monsters\n",
      "kid\n",
      "movie\n",
      "hell\n",
      "killer\n",
      "scary\n",
      "bugs\n",
      "gigantic\n",
      "crazy\n"
     ]
    }
   ],
   "source": [
    "get_sim('monster', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5169721245765686"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score('shrinks', 'grow', w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
