{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, pdb, csv\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from itertools import ifilter\n",
    "from random import randint\n",
    "from joblib import Parallel, delayed\n",
    "from torch.optim import lr_scheduler\n",
    "from random import shuffle\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glovedict(glove_path):\n",
    "    vocab_d = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            vocab_d[word] = np.array(list(map(float, vec.split())))\n",
    "            \n",
    "    return vocab_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample input: 2.0\t[NP]\tthis one\tthis dimension\t1.0,2.0,2.0,2.0,3.0\n",
    "def read_data(flName):    \n",
    "    with open(flName, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    pairs, max_len = [], 0\n",
    "    scores= []\n",
    "    for l in lines:\n",
    "        dt = l.split(\"\\t\")\n",
    "        score = float(dt[0])        \n",
    "        max_len = max(len(dt[2].split(\" \")), len(dt[3].split(\" \")), max_len)\n",
    "        pairs.append((score, dt[2], dt[3]))\n",
    "        scores.append(score)\n",
    "    scores = np.array(scores)\n",
    "    print(np.mean(scores), np.std(scores))\n",
    "    return pairs, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data_split(num):\n",
    "    flName = \"./human-labeled-data/ppdb-sample.tsv\"\n",
    "    pairs, max_len = read_data(flName)\n",
    "    # normalize pairs\n",
    "    # convert all p1-p2 to p1-{p i's}\n",
    "    pairs_dict = {}\n",
    "    for d in pairs:\n",
    "        s, p1, p2 = d\n",
    "        if p1 not in pairs_dict:\n",
    "            pairs_dict[p1] = []\n",
    "        pairs_dict[p1].append((s, p2))\n",
    "    \n",
    "    print(\"processing ppdb pairs count:\", len(pairs))\n",
    "    print(\"dict size:\", len(pairs_dict))\n",
    "    \n",
    "    indx = np.random.permutation(len(pairs_dict))[:num]\n",
    "    inv_idx = np.random.permutation(len(pairs_dict))[num:]\n",
    "    \n",
    "    val_pair_keys = [pairs_dict.keys()[indi] for indi in indx]\n",
    "    train_pair_keys = [pairs_dict.keys()[indi] for indi in inv_idx]\n",
    "    \n",
    "    return train_pair_keys, val_pair_keys, pairs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "class DAN(nn.Module):\n",
    "    # glove dict is a numpy matrix\n",
    "    def __init__(self):\n",
    "        super(DAN, self).__init__()\n",
    "        \"\"\"\n",
    "        self.word_emb = nn.Embedding(glove_tensor.size(0), hid_dim, requires_grad=False)\n",
    "        self.word_emb.weight.data.copy_(glove_tensor)\n",
    "        self.word_emb.weight.requires_grad = False\n",
    "        \"\"\"\n",
    "        self.relu = nn.ReLU()        \n",
    "        self.lin1 = nn.Linear(300, 300)\n",
    "        \n",
    "        self.batn1 = nn.BatchNorm1d(300, affine=False)\n",
    "        self.batn2 = nn.BatchNorm1d(300, affine=False)\n",
    "        self.batn3 = nn.BatchNorm1d(400, affine=False)\n",
    "        self.batn4 = nn.BatchNorm1d(100, affine=False)\n",
    "        \n",
    "        self.lin2 = nn.Linear(300, 300)\n",
    "        self.lin3 = nn.Linear(901, 400)\n",
    "        self.lin4 = nn.Linear(400, 100)\n",
    "        self.lin5 = nn.Linear(100, 1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.squeeze(1)\n",
    "        x2 = x2.squeeze(1)\n",
    "        # assume that they are converted to embeddings outside of the model\n",
    "        o1 = self.relu(self.batn2(self.lin2(self.relu(self.batn1(self.lin1(x1))))))\n",
    "        o2 = self.relu(self.batn2(self.lin2(self.relu(self.batn1(self.lin1(x2))))))\n",
    "        \n",
    "        cor = torch.sum(o1 * o2, 1, keepdim=True)\n",
    "        nxt = torch.cat([o1, o2, torch.abs(o1-o2), cor], 1)\n",
    "        score = self.lin5(self.relu(self.batn4(self.lin4(self.relu(self.batn3(self.lin3(nxt)))))))\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gdict = get_glovedict(\"glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(epochs, train_pair_keys, pairs_dict):\n",
    "    glove_path, dim = \"glove.840B.300d.txt\", 300\n",
    "    dan = DAN()    \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        dan.cuda()\n",
    "    \n",
    "    optimizer = optim.Adagrad(dan.parameters(), lr=0.0001)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        loss_vals, batchx1, batchx2, scores = [], [], [], []\n",
    "        for p, x1 in enumerate(train_pair_keys):\n",
    "            for score, x2 in pairs_dict[x1]:\n",
    "                vec1, vec2 = torch.zeros(dim), torch.zeros(dim)\n",
    "                x1s = x1.split(\" \")\n",
    "                for wrd in x1s:\n",
    "                    if wrd in gdict:\n",
    "                        vec1 += torch.from_numpy(gdict[wrd]).float()\n",
    "                if len(x1s) > 0:\n",
    "                    vec1 = vec1/len(x1s)\n",
    "\n",
    "                x2s = x2.split(\" \")\n",
    "                for wrd in x2s:\n",
    "                    if wrd in gdict:\n",
    "                        vec2 += torch.from_numpy(gdict[wrd]).float()            \n",
    "                if len(x2s) > 0:\n",
    "                    vec2 = vec2/len(x2s)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    vec2 = vec2.unsqueeze(0)\n",
    "                    vec1 = vec1.unsqueeze(0)\n",
    "\n",
    "                if len(batchx1) == BATCH_SIZE:\n",
    "                    x1v = torch.stack(batchx1, 0)\n",
    "                    x1v = Variable(x1v.cuda())\n",
    "                    x2v = torch.stack(batchx2, 0)\n",
    "                    x2v = Variable(x2v.cuda())\n",
    "\n",
    "                    score_t = Variable(torch.FloatTensor([scores]).cuda())\n",
    "\n",
    "                    prob = dan(x1v, x2v)\n",
    "                    loss = criterion(prob, score_t)\n",
    "                    loss_vals.append(loss.data[0])\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    batchx1, batchx2, scores = [], [], []\n",
    "\n",
    "                else:\n",
    "                    batchx2.append(vec2)\n",
    "                    batchx1.append(vec1)                \n",
    "                    scores.append(score)\n",
    "\n",
    "        print(\"after epoch {} loss is {} \".format(epoch, sum(loss_vals)*1.0/len(loss_vals)))    \n",
    "    return dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_pairs(dan, dim, val_pair_keys, pairs_dict):\n",
    "    model_results, actual_results = [], []\n",
    "    batchx1, batchx2, scores = [], [], []\n",
    "    for p, x1 in enumerate(val_pair_keys):\n",
    "        for score, x2 in pairs_dict[x1]:\n",
    "            \n",
    "            vec1, vec2 = torch.zeros(dim), torch.zeros(dim)\n",
    "\n",
    "            x1s = x1.split(\" \")\n",
    "            for wrd in x1s:\n",
    "                if wrd in gdict:\n",
    "                    vec1 += torch.from_numpy(gdict[wrd]).float()\n",
    "            if len(x1s) > 0:\n",
    "                vec1 = vec1/len(x1s)\n",
    "\n",
    "            x2s = x2.split(\" \")\n",
    "            for wrd in x2s:\n",
    "                if wrd in gdict:\n",
    "                    vec2 += torch.from_numpy(gdict[wrd]).float()            \n",
    "            if len(x2s) > 0:\n",
    "                vec2 = vec2/len(x2s)\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                vec2 = vec2.unsqueeze(0)\n",
    "                vec1 = vec1.unsqueeze(0)\n",
    "\n",
    "            if len(batchx1) == BATCH_SIZE:\n",
    "                x1v = torch.stack(batchx1, 0)\n",
    "                x1v = Variable(x1v.cuda())\n",
    "                x2v = torch.stack(batchx2, 0)\n",
    "                x2v = Variable(x2v.cuda())\n",
    "\n",
    "                score_t = Variable(torch.FloatTensor([scores]).cuda())\n",
    "\n",
    "                prob = dan(x1v, x2v)\n",
    "                model_results.extend(prob.data.cpu().numpy().tolist())\n",
    "                actual_results.extend(scores)\n",
    "\n",
    "                batchx1 = []\n",
    "                batchx2 = []\n",
    "                scores = []\n",
    "            else:\n",
    "                batchx2.append(vec2)\n",
    "                batchx1.append(vec1)                \n",
    "                scores.append(score)\n",
    "                \n",
    "    return actual_results, model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_pair_keys, val_pair_keys, pairs_dict = get_data_split(200)\n",
    "    dan = train(5, train_pair_keys, pairs_dict)\n",
    "    torch.save(dan.state_dict(), './dan.pth')\n",
    "    actual_r, model_r = evaluate_pairs(dan, 300, val_pair_keys, pairs_dict)\n",
    "    spr_ratio = spearmanr(actual_r, model_r)[0]\n",
    "    print(spr_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.6335002771898006, 1.1059656076675592)\n",
      "('processing ppdb pairs count:', 26456)\n",
      "('dict size:', 2286)\n",
      "after epoch 0 loss is 6.05497484168 \n",
      "after epoch 1 loss is 5.51465721564 \n",
      "after epoch 2 loss is 5.2302785033 \n",
      "after epoch 3 loss is 5.02024840323 \n",
      "after epoch 4 loss is 4.85025647898 \n",
      "0.299762963941\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validation(dan):\n",
    "    flName, dim = \"./human-labeled-data/ppdb-sample.tsv\", 300\n",
    "    pairs, max_len = read_data(flName)\n",
    "    print(\"processing ppdb pairs count:\", len(pairs))\n",
    "    indx = np.random.permutation(len(pairs))[:500]\n",
    "    inv_idx = np.random.permutation(len(pairs))[500:]\n",
    "    \n",
    "    val_pairs = [pairs[indi] for indi in indx]\n",
    "    train_pairs = [pairs[indi] for indi in inv_idx]\n",
    "    \n",
    "    model_results, actual_results = [], []\n",
    "    batchx1, batchx2, scores = [], [], []\n",
    "    for i, p in enumerate(val_pairs):\n",
    "        score, x1, x2 = p\n",
    "        vec1, vec2 = torch.zeros(dim), torch.zeros(dim)\n",
    "\n",
    "        x1s = x1.split(\" \")\n",
    "        for wrd in x1s:\n",
    "            if wrd in gdict:\n",
    "                vec1 += torch.from_numpy(gdict[wrd]).float()\n",
    "        if len(x1s) > 0:\n",
    "            vec1 = vec1/len(x1s)\n",
    "\n",
    "        x2s = x2.split(\" \")\n",
    "        for wrd in x2s:\n",
    "            if wrd in gdict:\n",
    "                vec2 += torch.from_numpy(gdict[wrd]).float()            \n",
    "        if len(x2s) > 0:\n",
    "            vec2 = vec2/len(x2s)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            vec2 = vec2.unsqueeze(0)\n",
    "            vec1 = vec1.unsqueeze(0)\n",
    "\n",
    "        if len(batchx1) == BATCH_SIZE:\n",
    "            x1 = torch.stack(batchx1, 0)\n",
    "            x1 = Variable(x1.cuda())\n",
    "            x2 = torch.stack(batchx2, 0)\n",
    "            x2 = Variable(x2.cuda())\n",
    "\n",
    "            score_t = Variable(torch.FloatTensor([scores]).cuda())\n",
    "\n",
    "            prob = dan(x1, x2)\n",
    "            model_results.extend(prob.data.cpu().numpy().tolist())\n",
    "            actual_results.extend(scores)\n",
    "\n",
    "            batchx1 = []\n",
    "            batchx2 = []\n",
    "            scores = []\n",
    "        else:\n",
    "            batchx2.append(vec2)\n",
    "            batchx1.append(vec1)                \n",
    "            scores.append(score)\n",
    "                \n",
    "    return actual_results, model_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
