{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, pdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from itertools import ifilter\n",
    "from random import randint\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import torch, pdb, csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 130000\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, hid_dim, pretrained=None):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        \n",
    "        #these are by intent to learn separate embedding matrices, we return word_emb\n",
    "        self.word_emb = nn.Embedding(vocab_size, hid_dim)\n",
    "        if pretrained is not None:\n",
    "            self.word_emb.weight.data.copy_(pretrained)\n",
    "        \n",
    "        self.context_emb = nn.Embedding(vocab_size, hid_dim)\n",
    "        if pretrained is not None:\n",
    "            self.context_emb.weight.data.copy_(pretrained)        \n",
    "        \n",
    "        self.sigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, wrd, cntxt, labels):\n",
    "        wrd_vec = self.word_emb(wrd) # N * 1 * D\n",
    "        cntxt_vec = self.context_emb(cntxt) # N * 5 * D\n",
    "        res = torch.bmm(wrd_vec, cntxt_vec.view(BATCH_SIZE, self.hid_dim, -1))\n",
    "        res = res.squeeze(1)\n",
    "        res = res * labels\n",
    "        res = self.sigmoid(res)\n",
    "        \n",
    "        # these are N * (1 + neg_exmpl) logsigmoid values\n",
    "        # for each mini-batch we have a probability score for the 5 contexts\n",
    "        # return res\n",
    "        \n",
    "        return (torch.sum(res)*-1.0)/res.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(wrd_indx, k, mat, descending=True):    \n",
    "    vec = mat[wrd_indx, :].unsqueeze(1)\n",
    "    othrs = torch.mm(mat, vec)\n",
    "    othrs, ind = torch.sort(othrs, 0, descending)\n",
    "    topk = ind[:k]\n",
    "    results = []\n",
    "    for i in range(topk.size()[0]):\n",
    "        results.append(topk[i][0])\n",
    "    return results\n",
    "\n",
    "def get_score(wrd1, wrd2, mat, word2index):\n",
    "    return torch.dot(mat[word2index[wrd1],:], mat[word2index[wrd2], :])\n",
    "\n",
    "def get_glovedict(glove_path):\n",
    "    vocab_d = set()\n",
    "    glove_vecs = {}\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            embedding = np.array([float(val) for val in vec.split(\" \")])\n",
    "            vocab_d.add(word)\n",
    "            glove_vecs[word] = embedding\n",
    "            \n",
    "    return vocab_d, glove_vecs\n",
    "    \n",
    "def get_gloveready(glove_path, vocab_size, dim, word2index):\n",
    "    pretrained_weight = torch.FloatTensor(vocab_size, dim)\n",
    "    fnd = 0\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            if word in word2index:\n",
    "                ind = word2index[word]\n",
    "                pretrained_weight[ind, :] = torch.from_numpy(np.array(list(map(float, vec.split()))))\n",
    "                fnd += 1\n",
    "\n",
    "    print('Found {0} words with glove vectors, total was {1}'.format(fnd, vocab_size))\n",
    "    return pretrained_weight\n",
    "\n",
    "def process_lines(data):\n",
    "    pairs, vocab = set(), {}\n",
    "    for cn, l in enumerate(data):\n",
    "        dt = l.split(\"|||\")\n",
    "        score = float(dt[3].split(\" \")[1].split(\"=\")[1])\n",
    "        if score < 3.1:\n",
    "            continue\n",
    "        wrd1, wrd2 = dt[1], dt[2]\n",
    "        wrd1, wrd2 = wrd1.strip(), wrd2.strip()\n",
    "\n",
    "        if \".pdf\" not in wrd1 and \".pdf\" not in wrd2 and wrd1.isalpha() and wrd2.isalpha():\n",
    "            sc = editdist_score(wrd1, wrd2)\n",
    "            if sc > min(len(wrd1), len(wrd2))/2:\n",
    "                if wrd1 + \" \" + wrd2 not in pairs and wrd2 + \" \" + wrd1 not in pairs:\n",
    "                    pairs.add(wrd1 + \" \" + wrd2)\n",
    "                    if wrd1 not in vocab:\n",
    "                        vocab[wrd1] = 1\n",
    "                    else:\n",
    "                        vocab[wrd1] += 1\n",
    "\n",
    "                    if wrd2 not in vocab:\n",
    "                        vocab[wrd2] = 1\n",
    "                    else:\n",
    "                        vocab[wrd2] += 1\n",
    "\n",
    "    return pairs, vocab\n",
    "\n",
    "def get_vocab(min_freq, flName=None, lines=None):\n",
    "    if flName is not None:\n",
    "        with open(flName) as fp:\n",
    "            lines = fp.readlines()\n",
    "    \n",
    "    return process_lines(lines)\n",
    "\n",
    "def get_chunks(lines, cn):\n",
    "    chunks = []\n",
    "    chunk_size = len(lines)//cn\n",
    "    for i in range(0, chunk_size*cn + 1):\n",
    "        chunk = lines[i*chunk_size:i*chunk_size + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def editdist_score(p1, p2):\n",
    "    n, m = len(p1), len(p2)\n",
    "    dp = [[0 for x in range(m+1)] for x in range(n+1)]\n",
    "\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            if i == 0:\n",
    "                dp[0][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][0] = i            \n",
    "            elif p1[i-1] == p2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n",
    "    return dp[n][m]\n",
    "\n",
    "def filter_data(pairs, word2index):\n",
    "    new_pairs = set()\n",
    "    fp = open(\"ppdb-processed.txt\", \"w\")\n",
    "    for line in pairs:        \n",
    "        p1, p2 = line.split(\" \")\n",
    "        if p1 in word2index and p2 in word2index:\n",
    "            new_pairs.add(p1 + \" \" + p2)\n",
    "            fp.write(line)\n",
    "            fp.write(\"\\n\")\n",
    "            \n",
    "    fp.close()\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_opposites():\n",
    "    oppos = {}\n",
    "    with open('sneha_antonyms.csv', 'r') as csvfile:\n",
    "        next(csvfile)\n",
    "        data = csv.reader(csvfile, delimiter=',')\n",
    "        for row in data:\n",
    "            if row[1] == 'antonym' and float(row[2]) > 0.85:\n",
    "                wrd1, wrd2 = row[3], row[4]\n",
    "                wrd1 = wrd1.strip().lower()\n",
    "                wrd2 = wrd2.strip().lower()\n",
    "                \n",
    "                if wrd1 not in oppos:\n",
    "                    oppos[wrd1] = []\n",
    "                oppos[wrd1].append(wrd2)\n",
    "                if wrd2 not in oppos:\n",
    "                    oppos[wrd2] = []\n",
    "                oppos[wrd2].append(wrd1)\n",
    "    return oppos\n",
    "\n",
    "def init_train(glove_path, dim, min_count, neg_exmpl):\n",
    "    g_vocab, glove_vecs = get_glovedict(glove_path)\n",
    "    pairs, tok_freq = get_vocab(min_count, flName=\"ppdb-2.0-l-lexical\")\n",
    "    opposites = load_opposites()\n",
    "    for wrd in opposites:\n",
    "        if wrd not in tok_freq:\n",
    "            tok_freq[wrd] = 0\n",
    "        tok_freq[wrd] += 1\n",
    "        \n",
    "        antos = opposites[wrd]\n",
    "        for awrd in antos:\n",
    "            if awrd not in tok_freq:\n",
    "                tok_freq[awrd] = 0\n",
    "            tok_freq[awrd] += 1\n",
    "            \n",
    "    vocab = set(tok_freq.keys())\n",
    "    vocab = vocab.intersection(g_vocab)\n",
    "\n",
    "    word2index, index2word = {}, {}\n",
    "    \n",
    "    for wrd in vocab:\n",
    "        if tok_freq[wrd] >= min_count:\n",
    "            index2word[len(index2word)] = wrd\n",
    "            word2index[wrd] = len(index2word) - 1\n",
    "        else:\n",
    "            tok_freq[wrd] = 0\n",
    "            \n",
    "    pairs = filter_data(pairs, word2index)\n",
    "    print(\"Data ready: {} {} {}\".format(len(index2word), len(pairs), len(vocab)))\n",
    "    return pairs, word2index, index2word, vocab, tok_freq, opposites, glove_vecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 27935 389421 27935\n"
     ]
    }
   ],
   "source": [
    "glove_path, dim, min_count, neg_exmpl = \"glove.6B.50d.txt\", 50, 1, 10\n",
    "pairs, word2index, index2word, vocab, tok_freq, opposites, glove_vecs = init_train(glove_path, dim, min_count, neg_exmpl)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file, k):\n",
    "    file = open(file,'r')\n",
    "    lines = file.readlines()\n",
    "    lines.pop(0)\n",
    "    examples = []\n",
    "    for i in lines:\n",
    "        i = i.strip()\n",
    "        i = i.lower()\n",
    "        if len(i) > 0:\n",
    "            i = i.split()\n",
    "            ex = (i[0], i[1], float(i[k]))\n",
    "            examples.append(ex)\n",
    "    return examples\n",
    "\n",
    "def getCorrelation(lines, We, word2index, glove_vecs):\n",
    "    gold, pred, skip = [], [], 0\n",
    "    for i in lines:\n",
    "        if i[0] not in word2index or i[1] not in word2index:\n",
    "            if i[0] not in glove_vecs or i[1] not in glove_vecs:                \n",
    "                skip += 1\n",
    "                continue            \n",
    "            else:\n",
    "                vec1 = glove_vecs[i[0]]\n",
    "                vec2 = glove_vecs[i[1]]\n",
    "        else:\n",
    "            vec1 = We[word2index[i[0]], :].cpu().numpy()\n",
    "            vec2 = We[word2index[i[1]], :].cpu().numpy()\n",
    "        pred.append(-1*cosine(vec1,vec2)+1)\n",
    "        gold.append(i[2])\n",
    "    print(\"Processed: {} total size was: {}\".format(len(lines) - skip, len(lines)))\n",
    "    return (spearmanr(pred,gold)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27935 words with glove vectors, total was 27935\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight = get_gloveready(glove_path, len(word2index), dim, word2index)\n",
    "pretrained_weight = torch.nn.functional.normalize(pretrained_weight)\n",
    "botk = {}\n",
    "for wrd in vocab:\n",
    "    res = get_sim(word2index[wrd], 10, pretrained_weight, False)\n",
    "    botk[wrd] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presuppose confers accrue accruals iias receptivity overemphasize sensitiveness confer underplay \n",
      "smashed ripped smashing blew tore crashing ripping blown cracked knocking hurled shattered slammed burned littered knocked torched pulled windshields chased thrown tossed strewn wrecked crushed trapped felled piled exploded blowing threw dumped dragged charred damaged burnt hauled rolled injuring rioters twisted onto gutted inside struck parked broken mangled blast grenades\n"
     ]
    }
   ],
   "source": [
    "for w in botk['smashed']:\n",
    "    print index2word[w],\n",
    "\n",
    "print(\"\")    \n",
    "for w in get_sim(word2index['smashed'], 50, pretrained_weight):\n",
    "    print index2word[w],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glove_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mdl = Word2Vec(len(index2word), dim)    \n",
    "mdl.load_state_dict(torch.load('./mdl_skipgme8.pth', map_location=lambda storage, loc: storage))\n",
    "w2vmat = mdl.word_emb.weight.data.cpu()\n",
    "wnorm = torch.norm(w2vmat, 2, 1, True)\n",
    "w2vmat = w2vmat/wnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77895587682724"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(\"happy\", \"joy\", w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "simlex_lines = read_data('./SimLex-999/SimLex-999.txt', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('old', 'new', 1.58)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simlex_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 635 total size was: 999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.377914896897318"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCorrelation(simlex_lines, w2vmat, word2index, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6890631914138794"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(\"happy\", \"sad\", pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat ate eating cook meal feed cooked vegetables fish eggs\n"
     ]
    }
   ],
   "source": [
    "for w in get_sim(word2index['eat'], 10, w2vmat):\n",
    "    print index2word[w], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 999 total size was: 999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.26457921929908129"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCorrelation(simlex_lines, pretrained_weight, word2index, glove_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ws353sim_lines = read_data('./wordsim353/wordsim_simg.txt', 2)\n",
    "ws353rel_lines = read_data('./wordsim353/wordsim_relg.txt', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tiger', 'tiger', 10.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws353sim_lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 202 total size was: 202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54665539293448751"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCorrelation(ws353sim_lines, w2vmat, word2index, glove_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 251 total size was: 251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.41002426760157962"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCorrelation(ws353rel_lines, w2vmat, word2index, glove_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 202 total size was: 202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5719907855159363"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCorrelation(ws353sim_lines, pretrained_weight, word2index, glove_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 251 total size was: 251\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.46421022514164895"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getCorrelation(ws353rel_lines, pretrained_weight, word2index, glove_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear afraid fearing doubt anger threat concern cause reason danger\n"
     ]
    }
   ],
   "source": [
    "for w in get_sim(word2index['fear'], 10, w2vmat):\n",
    "    print index2word[w], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "reinvigorated\n",
      "imprudent\n",
      "deception\n",
      "ornamental\n",
      "vindicating\n",
      "induces\n",
      "presentations\n",
      "shotguns\n",
      "buried\n"
     ]
    }
   ],
   "source": [
    "get_sim('eat', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eating\n",
      "ate\n",
      "eaten\n",
      "eats\n",
      "cooked\n",
      "fish\n",
      "vegetables\n",
      "feed\n",
      "eggs\n"
     ]
    }
   ],
   "source": [
    "get_sim('eat', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "worry\n",
      "danger\n",
      "fears\n",
      "anger\n",
      "blame\n",
      "fearing\n",
      "worried\n",
      "threatening\n",
      "cause\n"
     ]
    }
   ],
   "source": [
    "get_sim('fear', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "danger\n",
      "worried\n",
      "afraid\n",
      "concern\n",
      "threat\n",
      "anger\n",
      "feared\n",
      "fearing\n",
      "reason\n"
     ]
    }
   ],
   "source": [
    "get_sim('fear', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "always\n",
      "something\n",
      "so\n",
      "really\n",
      "little\n",
      "i\n",
      "maybe\n",
      "you\n",
      "me\n"
     ]
    }
   ],
   "source": [
    "get_sim('like', 10, w2vmat, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "like\n",
      "look\n",
      "well\n",
      "come\n",
      "so\n",
      "how\n",
      "little\n",
      "big\n",
      "everything\n",
      "lot\n"
     ]
    }
   ],
   "source": [
    "get_sim('like', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
