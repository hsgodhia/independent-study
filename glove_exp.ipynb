{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch, pdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from itertools import ifilter\n",
    "from IPython.core.debugger import set_trace\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim(wrd, k, mat, word2index):\n",
    "    if wrd not in word2index:\n",
    "        return None\n",
    "    vec = mat[word2index[wrd], :].unsqueeze(1)\n",
    "    othrs = torch.mm(mat, vec)\n",
    "    othrs, ind = torch.sort(othrs, 0, descending=True)\n",
    "    topk = ind[:k]\n",
    "    for i in range(topk.size()[0]):\n",
    "        print(index2word[topk[i][0]])\n",
    "\n",
    "def get_glovedict(glove_path):\n",
    "    vocab_d = set()\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            vocab_d.add(word)\n",
    "            \n",
    "    return vocab_d\n",
    "    \n",
    "def get_gloveready(glove_path, vocab_size, dim, word2index):\n",
    "    pretrained_weight = torch.FloatTensor(vocab_size, dim)\n",
    "    fnd = 0\n",
    "    with open(glove_path) as f:\n",
    "        for line in f:\n",
    "            word, vec = line.split(' ', 1)\n",
    "            word = word.strip().lower()\n",
    "            if word in word2index:\n",
    "                ind = word2index[word]\n",
    "                pretrained_weight[ind, :] = torch.from_numpy(np.array(list(map(float, vec.split()))))\n",
    "                fnd += 1\n",
    "\n",
    "    print('Found {0} words with glove vectors, total was {1}'.format(fnd, vocab_size))\n",
    "    return pretrained_weight\n",
    "\n",
    "def process_lines(data):\n",
    "    pairs, vocab = set(), {}\n",
    "    for cn, l in enumerate(data):\n",
    "        dt = l.split(\"|||\")\n",
    "        score = float(dt[3].split(\" \")[1].split(\"=\")[1])\n",
    "        if score < 3.3:\n",
    "            continue\n",
    "        wrd1, wrd2 = dt[1], dt[2]\n",
    "        wrd1, wrd2 = wrd1.strip(), wrd2.strip()\n",
    "\n",
    "        if \".pdf\" not in wrd1 and \".pdf\" not in wrd2 and wrd1.isalpha() and wrd2.isalpha():\n",
    "            sc = editdist_score(wrd1, wrd2)\n",
    "            if sc > min(len(wrd1), len(wrd2))/2 + 2:\n",
    "                if wrd1 + \" \" + wrd2 not in pairs and wrd2 + \" \" + wrd1 not in pairs:\n",
    "                    pairs.add(wrd1 + \" \" + wrd2)\n",
    "                    if wrd1 not in vocab:\n",
    "                        vocab[wrd1] = 1\n",
    "                    else:\n",
    "                        vocab[wrd1] += 1\n",
    "\n",
    "                    if wrd2 not in vocab:\n",
    "                        vocab[wrd2] = 1\n",
    "                    else:\n",
    "                        vocab[wrd2] += 1\n",
    "\n",
    "    return pairs, vocab\n",
    "\n",
    "def get_vocab(min_freq, flName=None, lines=None):\n",
    "    if flName is not None:\n",
    "        with open(flName) as fp:\n",
    "            lines = fp.readlines()\n",
    "    \n",
    "    return process_lines(lines)\n",
    "\n",
    "def get_chunks(lines, cn):\n",
    "    chunks = []\n",
    "    chunk_size = len(lines)//cn\n",
    "    for i in range(0, chunk_size*cn + 1):\n",
    "        chunk = lines[i*chunk_size:i*chunk_size + chunk_size]\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "def editdist_score(p1, p2):\n",
    "    n, m = len(p1), len(p2)\n",
    "    dp = [[0 for x in range(m+1)] for x in range(n+1)]\n",
    "\n",
    "    for i in range(n+1):\n",
    "        for j in range(m+1):\n",
    "            if i == 0:\n",
    "                dp[0][j] = j\n",
    "            elif j == 0:\n",
    "                dp[i][0] = i            \n",
    "            elif p1[i-1] == p2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]\n",
    "            else:\n",
    "                dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n",
    "    return dp[n][m]\n",
    "\n",
    "def filter_data(pairs, word2index):\n",
    "    new_pairs = set()\n",
    "    fp = open(\"ppdb-processed.txt\", \"w\")\n",
    "    for line in pairs:        \n",
    "        p1, p2 = line.split(\" \")\n",
    "        if p1 in word2index and p2 in word2index:\n",
    "            new_pairs.add(p1 + \" \" + p2)\n",
    "            fp.write(line)\n",
    "            fp.write(\"\\n\")\n",
    "            \n",
    "    fp.close()\n",
    "    return new_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready: 21232 193269 21232\n"
     ]
    }
   ],
   "source": [
    "glove_path, dim, min_count, neg_exmpl = \"glove.6B.50d.txt\", 50, 1, 10\n",
    "g_vocab = get_glovedict(glove_path)\n",
    "pairs, tok_freq = get_vocab(min_count, flName=\"ppdb-2.0-l-lexical\")\n",
    "\n",
    "vocab = set(tok_freq.keys())\n",
    "vocab = vocab.intersection(g_vocab)\n",
    "word2index, index2word = {}, {}\n",
    "\n",
    "for wrd in vocab:\n",
    "    if tok_freq[wrd] >= min_count:\n",
    "        index2word[len(index2word)] = wrd\n",
    "        word2index[wrd] = len(index2word) - 1\n",
    "    else:\n",
    "        tok_freq[wrd] = 0\n",
    "\n",
    "pairs = filter_data(pairs, word2index)\n",
    "vocab_size = len(index2word)\n",
    "print(\"Data ready: {} {} {}\".format(vocab_size, len(pairs), len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21232 words with glove vectors, total was 21232\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight = get_gloveready(glove_path, vocab_size, dim, word2index)\n",
    "pretrained_weight = torch.nn.functional.normalize(pretrained_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "young\n",
      "who\n",
      "friends\n",
      "fellow\n",
      "younger\n",
      "man\n",
      "parents\n",
      "friend\n",
      "couple\n",
      "boys\n"
     ]
    }
   ],
   "source": [
    "get_sim('young', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occur\n",
      "occurring\n",
      "occurs\n",
      "arise\n",
      "affect\n",
      "affected\n",
      "occurrence\n",
      "possibly\n",
      "cause\n",
      "due\n"
     ]
    }
   ],
   "source": [
    "get_sim('occur', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summer\n",
      "winter\n",
      "beginning\n",
      "starting\n",
      "day\n",
      "during\n",
      "days\n",
      "year\n",
      "next\n",
      "years\n"
     ]
    }
   ],
   "source": [
    "get_sim('summer', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eat\n",
      "eating\n",
      "ate\n",
      "eaten\n",
      "eats\n",
      "cooked\n",
      "fish\n",
      "vegetables\n",
      "feed\n",
      "eggs\n"
     ]
    }
   ],
   "source": [
    "get_sim('eat', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fear\n",
      "worry\n",
      "danger\n",
      "fears\n",
      "anger\n",
      "blame\n",
      "fearing\n",
      "worried\n",
      "threatening\n",
      "cause\n"
     ]
    }
   ],
   "source": [
    "get_sim('fear', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "ho\n",
      "ai\n",
      "tu\n",
      "yo\n",
      "ya\n",
      "techs\n",
      "wow\n",
      "ok\n",
      "hurts\n"
     ]
    }
   ],
   "source": [
    "get_sim('hi', 10, pretrained_weight, word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salary\n",
      "salaries\n",
      "payroll\n",
      "bonuses\n",
      "minimum\n",
      "pay\n",
      "payments\n",
      "guaranteed\n",
      "paid\n",
      "paying\n"
     ]
    }
   ],
   "source": [
    "get_sim('salary', 10, pretrained_weight, word2index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
